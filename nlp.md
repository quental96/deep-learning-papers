# Seminal Papers

# NLP

## Quick Links
  - [1997](#1997) (1 paper)
  - [2003](#2003) (1 paper)
  - [2004](#2004) (1 paper)
  - [2005](#2005) (1 paper)
  - [2010](#2010) (1 paper)
  - [2011](#2011) (1 paper)
  - [2013](#2013) (2 papers)
  - [2014](#2014) (3 papers)
  - [2015](#2015) (3 papers)
  - [2016](#2016) (3 papers)
  - [2017](#2017) (4 papers)
  - [2018](#2018) (8 papers)
  - [2019](#2019) (14 papers)
  - [2020](#2020) (14 papers)
  - [2021](#2021) (12 papers)
  - [2022](#2022) (20 papers)
  - [2023](#2023) (16 papers)
  - [2024](#2024) (7 papers)

## Details

### 1997

- **Long Short-Term Memory**
  - **Authors**: Sepp Hochreiter, Jürgen Schmidhuber
  - **Description**: Introduces Long Short-Term Memory (LSTM) networks, addressing the vanishing gradient problem in recurrent neural networks (RNNs) and enabling better long-term sequence learning.
  - **Link**: [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)

### 2003

- **A Neural Probabilistic Language Model**
  - **Authors**: Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Janvin
  - **Description**: Proposes a neural probabilistic language model that learns distributed representations for words and predicts the probability of a sequence of words.
  - **Link**: [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

### 2004

- **ROUGE: a Package for Automatic Evaluation of Summaries**
  - **Authors**: Chin-Yew Lin
  - **Description**: Introduces ROUGE, a set of metrics for evaluating automatic summarization and machine translation by comparing generated summaries against reference summaries.
  - **Link**: [ROUGE](https://www.aclweb.org/anthology/W04-1013)

### 2005

- **METEOR: an Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments**
  - **Authors**: Alon Lavie, Abhaya Agarwal
  - **Description**: Presents METEOR, an evaluation metric for machine translation that improves correlation with human judgments by considering synonymy and stemming.
  - **Link**: [METEOR](https://www.aclweb.org/anthology/W05-0909)

### 2010

- **Recurrent Neural Network Based Language Model**
  - **Authors**: Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocký, Sanjeev Khudanpur
  - **Description**: Proposes a language model based on recurrent neural networks (RNNs) that outperforms traditional n-gram models on various tasks.
  - **Link**: [RNN Based Language Model](https://www.fit.vut.cz/research/publication-file/10321/Mikolov-TR2010.pdf)

### 2011

- **Generating Text with Recurrent Neural Networks**
  - **Authors**: Alex Graves
  - **Description**: Demonstrates how recurrent neural networks (RNNs) can be used for generating text, including handwriting, speech, and music.
  - **Link**: [Generating Text with RNNs](https://arxiv.org/abs/1308.0850)

### 2013

- **Efficient Estimation of Word Representations in Vector Space**
  - **Authors**: Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean
  - **Description**: Introduces Word2Vec, a set of models for learning word embeddings efficiently using continuous bag-of-words (CBOW) and skip-gram architectures.
  - **Link**: [Word2Vec](https://arxiv.org/abs/1301.3781)

- **Distributed Representations of Words and Phrases and Their Compositionality**
  - **Authors**: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean
  - **Description**: Extends Word2Vec to capture more complex linguistic structures, enabling the model to learn distributed representations of phrases.
  - **Link**: [Distributed Representations](https://arxiv.org/abs/1310.4546)

### 2014

- **On the Properties of Neural Machine Translation: Encoder–Decoder Approaches**
  - **Authors**: Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, Yoshua Bengio
  - **Description**: Analyzes the properties of neural machine translation models using encoder-decoder architectures, focusing on the advantages of these models over traditional methods.
  - **Link**: [Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)

- **GloVe: Global Vectors for Word Representation**
  - **Authors**: Jeffrey Pennington, Richard Socher, Christopher D. Manning
  - **Description**: Proposes GloVe, a method for learning word embeddings by aggregating global word-word co-occurrence statistics from a corpus.
  - **Link**: [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)

- **Sequence to Sequence Learning with Neural Networks**
  - **Authors**: Ilya Sutskever, Oriol Vinyals, Quoc V. Le
  - **Description**: Introduces sequence-to-sequence (Seq2Seq) learning using neural networks, which maps input sequences to output sequences, enabling applications like machine translation.
  - **Link**: [Seq2Seq](https://arxiv.org/abs/1409.3215)

- **Learning Phrase Representations Using RNN Encoder–Decoder for Statistical Machine Translation**
  - **Authors**: Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, Yoshua Bengio
  - **Description**: Proposes an RNN encoder-decoder model for learning phrase representations, improving statistical machine translation performance.
  - **Link**: [RNN Encoder-Decoder](https://arxiv.org/abs/1406.1078)

### 2015

- **Neural Machine Translation by Jointly Learning to Align and Translate**
  - **Authors**: Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio
  - **Description**: Introduces the attention mechanism in neural machine translation, allowing models to focus on relevant parts of the input sequence during translation.
  - **Link**: [Align and Translate](https://arxiv.org/abs/1409.0473)

- **Effective Approaches to Attention-based Neural Machine Translation**
  - **Authors**: Minh-Thang Luong, Hieu Pham, Christopher D. Manning
  - **Description**: Explores different attention mechanisms in neural machine translation, demonstrating significant improvements over traditional methods.
  - **Link**: [Attention-based NMT](https://arxiv.org/abs/1508.04025)

- **Skip-Thought Vectors**
  - **Authors**: Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, Sanja Fidler
  - **Description**: Proposes Skip-Thought Vectors, a method for learning sentence embeddings by training an encoder-decoder model to predict surrounding sentences.
  - **Link**: [Skip-Thought Vectors](https://arxiv.org/abs/1506.06726)

### 2016

- **Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation**
  - **Authors**: Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean
  - **Description**: Describes Google's Neural Machine Translation (GNMT) system, which significantly improves translation quality by using an end-to-end learning framework.
  - **Link**: [Google’s Neural Machine Translation System](https://arxiv.org/abs/1609.08144)

- **Neural Machine Translation of Rare Words with Subword Units**
  - **Authors**: Rico Sennrich, Barry Haddow, Alexandra Birch
  - **Description**: Proposes a method for translating rare words in neural machine translation by segmenting words into subword units, improving translation accuracy.
  - **Link**: [Translation of Rare Words](https://arxiv.org/abs/1508.07909)

- **HyperNetworks**
  - **Authors**: David Ha, Andrew Dai, Quoc V. Le
  - **Description**: Introduces HyperNetworks, which generate the weights of another neural network, enabling dynamic adaptation and parameter efficiency.
  - **Link**: [HyperNetworks](https://arxiv.org/abs/1609.09106)

### 2017

- **Attention is All You Need**
  - **Authors**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin
  - **Description**: Proposes the Transformer model, which relies entirely on attention mechanisms, significantly improving performance and efficiency in machine translation tasks.
  - **Link**: [Attention is All You Need](https://arxiv.org/abs/1706.03762)

- **Outrageously Large Neural Networks: the Sparsely-Gated Mixture-of-Experts Layer**
  - **Authors**: Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey Hinton, Jeff Dean
  - **Description**: Introduces the Sparsely-Gated Mixture-of-Experts (MoE) layer, which scales neural networks to unprecedented sizes by activating only a subset of the network's parameters.
  - **Link**: [Sparsely-Gated MoE](https://arxiv.org/abs/1701.06538)

- **Using the Output Embedding to Improve Language Models**
  - **Authors**: Paul J. Werbos, Ilya Sutskever
  - **Description**: Explores how leveraging the output embedding matrix during training can improve the performance of language models.
  - **Link**: [Improving Language Models](https://arxiv.org/abs/1608.05859)

- **Enriching Word Vectors with Subword Information**
  - **Authors**: Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov
  - **Description**: Enhances word vectors by incorporating subword information, capturing more semantic and syntactic nuances.
  - **Link**: [Subword Information](https://arxiv.org/abs/1607.04606)

### 2018

- **Deep Contextualized Word Representations**
  - **Authors**: Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer
  - **Description**: Introduces ELMo, a model that produces deep contextualized word representations, significantly improving performance on various NLP tasks.
  - **Link**: [ELMo](https://arxiv.org/abs/1802.05365)

- **Improving Language Understanding by Generative Pre-Training**
  - **Authors**: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever
  - **Description**: Proposes the GPT model, which pre-trains a transformer network on a large corpus and fine-tunes it for specific tasks, achieving state-of-the-art results in language understanding.
  - **Link**: [Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

- **SentencePiece: a Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing**
  - **Authors**: Taku Kudo, John Richardson
  - **Description**: Introduces SentencePiece, a language-independent tokenizer and detokenizer that segments text into subwords, improving the performance of neural text processing models.
  - **Link**: [SentencePiece](https://arxiv.org/abs/1808.06226)

- **Self-Attention with Relative Position Representations**
  - **Authors**: Peter Shaw, Jakob Uszkoreit, Ashish Vaswani
  - **Description**: Enhances the Transformer model by incorporating relative position representations into the self-attention mechanism.
  - **Link**: [Relative Position Representations](https://arxiv.org/abs/1803.02155)

- **Blockwise Parallel Decoding for Deep Autoregressive Models**
  - **Authors**: Maxim Krikun, Mohammad Norouzi, Samy Bengio, Orhan Firat, Melvin Johnson, Wolfgang Macherey, Macduff Hughes, Jeffrey Dean, Noam Shazeer, Yonghui Wu
  - **Description**: Proposes a method for parallelizing the decoding process in deep autoregressive models, significantly speeding up generation times.
  - **Link**: [Blockwise Parallel Decoding](https://arxiv.org/abs/1804.02610)

- **Universal Language Model Fine-tuning for Text Classification**
  - **Authors**: Jeremy Howard, Sebastian Ruder
  - **Description**: Introduces ULMFiT, a transfer learning method that fine-tunes a pre-trained language model for text classification tasks, achieving state-of-the-art results.
  - **Link**: [ULMFiT](https://arxiv.org/abs/1801.06146)

- **Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models**
  - **Authors**: Ashwin K. Vijayakumar, Michael Cogswell, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra
  - **Description**: Proposes Diverse Beam Search (DBS), an algorithm for generating diverse sets of solutions from neural sequence models.
  - **Link**: [Diverse Beam Search](https://arxiv.org/abs/1610.02424)

- **MS MARCO: a Human Generated MAchine Reading COmprehension Dataset**
  - **Authors**: Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder
  - **Description**: Introduces MS MARCO, a large-scale dataset for machine reading comprehension and question answering, with human-generated questions and answers.
  - **Link**: [MS MARCO](https://arxiv.org/abs/1611.09268)

### 2019

- **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**
  - **Authors**: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
  - **Description**: Proposes BERT, a model that pre-trains deep bidirectional transformers by jointly conditioning on both left and right contexts in all layers, achieving state-of-the-art results on various NLP tasks.
  - **Link**: [BERT](https://arxiv.org/abs/1810.04805)

- **RoBERTa: a Robustly Optimized BERT Pretraining Approach**
  - **Authors**: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov
  - **Description**: Introduces RoBERTa, which optimizes BERT pretraining by using more data, larger batch sizes, and training for longer periods, resulting in improved performance.
  - **Link**: [RoBERTa](https://arxiv.org/abs/1907.11692)

- **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension**
  - **Authors**: Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer
  - **Description**: Introduces BART, a denoising autoencoder for pretraining sequence-to-sequence models, achieving state-of-the-art results on text generation and comprehension tasks.
  - **Link**: [BART](https://arxiv.org/abs/1910.13461)

- **DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter**
  - **Authors**: Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf
  - **Description**: Introduces DistilBERT, a lighter and faster version of BERT achieved through knowledge distillation, retaining 97% of BERT's language understanding while being 60% faster.
  - **Link**: [DistilBERT](https://arxiv.org/abs/1910.01108)

- **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context**
  - **Authors**: Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov
  - **Description**: Proposes Transformer-XL, a model that captures long-term dependencies in language by introducing recurrence mechanisms to transformer models.
  - **Link**: [Transformer-XL](https://arxiv.org/abs/1901.02860)

- **XLNet: Generalized Autoregressive Pretraining for Language Understanding**
  - **Authors**: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le
  - **Description**: Introduces XLNet, which combines the advantages of autoregressive models and BERT, improving performance on various NLP tasks.
  - **Link**: [XLNet](https://arxiv.org/abs/1906.08237)

- **Adaptive Input Representations for Neural Language Modeling**
  - **Authors**: Edouard Grave, Myle Ott, Arthur Szlam, Marc’Aurelio Ranzato
  - **Description**: Presents a method for adaptive input representations in neural language models to better handle rare and common words alike.
  - **Link**: [Adaptive Input Representations](https://arxiv.org/abs/1904.09437)

- **Attention Interpretability Across NLP Tasks**
  - **Authors**: Swabha Swayamdipta, Roy Schwartz, Samuel R. Bowman
  - **Description**: Analyzes the interpretability of attention mechanisms across different NLP tasks, providing insights into how models make decisions.
  - **Link**: [Attention Interpretability](https://arxiv.org/abs/1909.11218)

- **Grad-CAM: Visual Explanations from Deep Networks Via Gradient-based Localization**
  - **Authors**: Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra
  - **Description**: Introduces Grad-CAM, a technique for producing visual explanations for decisions from deep networks using gradient-based localization.
  - **Link**: [Grad-CAM](https://arxiv.org/abs/1610.02391)

- **Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond**
  - **Authors**: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov
  - **Description**: Proposes multilingual sentence embeddings that enable zero-shot cross-lingual transfer, improving performance on multilingual NLP tasks.
  - **Link**: [Multilingual Sentence Embeddings](https://arxiv.org/abs/1911.02116)

- **GLUE: a Multi-Task Benchmark and Analysis Platform for Natural Language Understanding**
  - **Authors**: Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman
  - **Description**: Introduces GLUE, a benchmark for evaluating the performance of models across various natural language understanding tasks.
  - **Link**: [GLUE](https://arxiv.org/abs/1804.07461)

- **Parameter-Efficient Transfer Learning for NLP**
  - **Authors**: Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly
  - **Description**: Discusses methods for parameter-efficient transfer learning in NLP, reducing the number of parameters needed to fine-tune models.
  - **Link**: [Parameter-Efficient Transfer Learning](https://arxiv.org/abs/1902.00751)

- **Cross-lingual Language Model Pretraining**
  - **Authors**: Alexis Conneau, Guillaume Lample
  - **Description**: Proposes XLM, a cross-lingual pretraining approach for language models that significantly improves performance on multilingual tasks.
  - **Link**: [Cross-lingual Pretraining](https://arxiv.org/abs/1901.07291)

- **MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance**
  - **Authors**: Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, Steffen Eger
  - **Description**: Introduces MoverScore, a text generation evaluation metric that uses contextualized embeddings and Earth Mover Distance for improved evaluation.
  - **Link**: [MoverScore](https://arxiv.org/abs/1909.02622)

- **Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data**
  - **Authors**: Mariusz Trofimov, Ivan Rubachev, Balandin Landa, Sergey I. Nikolenko, Alexander Kropotov, Valery Strijov, Dmitry Vetrov
  - **Description**: Proposes NODE, a method for integrating decision trees with neural networks for improved performance on tabular data.
  - **Link**: [NODE](https://arxiv.org/abs/1909.06312)

- **Latent Retrieval for Weakly Supervised Open Domain Question Answering**
  - **Authors**: Gautier Izacard, Edouard Grave
  - **Description**: Presents a method for latent retrieval in weakly supervised open-domain question answering, improving retrieval accuracy and performance.
  - **Link**: [Latent Retrieval](https://arxiv.org/abs/1910.12073)

- **Multi-Stage Document Ranking with BERT**
  - **Authors**: Nandan Thakur, Nils Reimers, Johannes Daxenberger, Iryna Gurevych
  - **Description**: Proposes a multi-stage document ranking framework using BERT, achieving state-of-the-art results on several benchmarks.
  - **Link**: [Document Ranking](https://arxiv.org/abs/1910.14424)

- **Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts**
  - **Authors**: Hongwei Wang, Jure Leskovec
  - **Description**: Introduces a model that uses multi-gate mixture-of-experts to capture task relationships in multi-task learning.
  - **Link**: [Multi-gate Mixture-of-Experts](https://arxiv.org/abs/2002.06305)

- **Synthetic QA Corpora Generation with Roundtrip Consistency**
  - **Authors**: Gautier Izacard, Edouard Grave
  - **Description**: Proposes a method for generating synthetic question-answering corpora using roundtrip consistency, improving QA model performance.
  - **Link**: [Synthetic QA](https://arxiv.org/abs/2004.15012)

- **Towards VQA Models That Can Read**
  - **Authors**: D. Teney, E. Abbasnejad, A. Hengel
  - **Description**: Introduces models that improve Visual Question Answering (VQA) by incorporating reading comprehension capabilities.
  - **Link**: [VQA Models That Can Read](https://arxiv.org/abs/2004.14267)

### 2020

- **Language Models are Few-Shot Learners**
  - **Authors**: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei
  - **Description**: Demonstrates that GPT-3, a large-scale language model, can achieve strong performance on a wide range of tasks with few-shot learning.
  - **Link**: [GPT-3](https://arxiv.org/abs/2005.14165)

- **Longformer: the Long-Document Transformer**
  - **Authors**: Iz Beltagy, Matthew E. Peters, Arman Cohan
  - **Description**: Introduces Longformer, a transformer model designed to handle long documents by using sparse attention mechanisms.
  - **Link**: [Longformer](https://arxiv.org/abs/2004.05150)

- **Big Bird: Transformers for Longer Sequences**
  - **Authors**: Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed
  - **Description**: Proposes Big Bird, a sparse-attention-based transformer that extends the context window for handling longer sequences efficiently.
  - **Link**: [Big Bird](https://arxiv.org/abs/2007.14062)

- **Beyond Accuracy: Behavioral Testing of NLP Models with CheckList**
  - **Authors**: Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh
  - **Description**: Introduces CheckList, a methodology for evaluating NLP models by testing their behavior in various scenarios to identify strengths and weaknesses.
  - **Link**: [CheckList](https://arxiv.org/abs/2005.04118)

- **The Curious Case of Neural Text Degeneration**
  - **Authors**: Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi
  - **Description**: Analyzes neural text degeneration, proposing nucleus sampling to mitigate issues like repetition and incoherence in text generation.
  - **Link**: [Neural Text Degeneration](https://arxiv.org/abs/1904.09751)

- **ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators**
  - **Authors**: Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning
  - **Description**: Introduces ELECTRA, a pre-training method that trains text encoders to distinguish real input tokens from corrupted ones, achieving better efficiency and performance.
  - **Link**: [ELECTRA](https://arxiv.org/abs/2003.10555)

- **TinyBERT: Distilling BERT for Natural Language Understanding**
  - **Authors**: Jiao X., Yin Y., Shang L., Jiang X., Chen X., Li L., Wang F., Liu Q.
  - **Description**: Proposes TinyBERT, a distilled version of BERT that reduces model size and computation while retaining performance.
  - **Link**: [TinyBERT](https://arxiv.org/abs/1909.10351)

- **MPNet: Masked and Permuted Pre-training for Language Understanding**
  - **Authors**: Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu
  - **Description**: Introduces MPNet, which integrates masked and permuted language modeling to capture better dependencies among words.
  - **Link**: [MPNet](https://arxiv.org/abs/2004.09297)

- **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**
  - **Authors**: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
  - **Description**: Proposes T5, a model that frames all NLP tasks as text-to-text tasks, achieving state-of-the-art results across various benchmarks.
  - **Link**: [T5](https://arxiv.org/abs/1910.10683)

- **Scaling Laws for Neural Language Models**
  - **Authors**: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei
  - **Description**: Identifies scaling laws that describe how performance improves as model size, dataset size, and computation increase, guiding efficient resource allocation.
  - **Link**: [Scaling Laws](https://arxiv.org/abs/2001.08361)

- **Unsupervised Cross-lingual Representation Learning at Scale**
  - **Authors**: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov
  - **Description**: Proposes a method for unsupervised cross-lingual representation learning using a large-scale multilingual corpus, achieving strong performance on cross-lingual tasks.
  - **Link**: [Unsupervised Cross-lingual Representation Learning](https://arxiv.org/abs/1911.02116)

- **SpanBERT: Improving Pre-training by Representing and Predicting Spans**
  - **Authors**: Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy
  - **Description**: Proposes SpanBERT, which pre-trains BERT by predicting spans of text rather than individual tokens, enhancing its ability to handle span-based tasks.
  - **Link**: [SpanBERT](https://arxiv.org/abs/1907.10529)

- **Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning**
  - **Authors**: Noah Fiedel, Ethan Dyer
  - **Description**: Investigates the intrinsic dimensionality of language model fine-tuning, providing insights into why certain fine-tuning strategies are effective.
  - **Link**: [Intrinsic Dimensionality](https://arxiv.org/abs/2004.07676)

- **Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval**
  - **Authors**: Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Jianfeng Gao, Arnold Overwijk
  - **Description**: Proposes ANCE, a method for dense text retrieval that uses approximate nearest neighbor search to improve the quality of negative samples.
  - **Link**: [ANCE](https://arxiv.org/abs/2007.00808)

- **Document Ranking with a Pretrained Sequence-to-Sequence Model**
  - **Authors**: Nandan Thakur, Nils Reimers, Johannes Daxenberger, Iryna Gurevych
  - **Description**: Uses a pre-trained sequence-to-sequence model for document ranking, achieving state-of-the-art performance on several benchmarks.
  - **Link**: [Document Ranking](https://arxiv.org/abs/1910.14424)

- **ColBERT: Efficient and Effective Passage Search Via Contextualized Late Interaction Over BERT**
  - **Authors**: Omar Khattab, Matei Zaharia
  - **Description**: Introduces ColBERT, a model for efficient and effective passage search using contextualized late interaction over BERT embeddings.
  - **Link**: [ColBERT](https://arxiv.org/abs/2004.12832)

- **REALM: Retrieval-Augmented Language Model Pre-Training**
  - **Authors**: Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, Percy Liang
  - **Description**: Proposes REALM, a retrieval-augmented language model that improves language understanding by incorporating retrieved documents during pre-training.
  - **Link**: [REALM](https://arxiv.org/abs/2002.08909)

- **Linformer: Self-Attention with Linear Complexity**
  - **Authors**: Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, Hao Ma
  - **Description**: Proposes Linformer, an attention mechanism with linear complexity, making transformers more efficient for long sequences.
  - **Link**: [Linformer](https://arxiv.org/abs/2006.04768)

- **BLEURT: Learning Robust Metrics for Text Generation**
  - **Authors**: Thibault Sellam, Dipanjan Das, Ankur P. Parikh
  - **Description**: Introduces BLEURT, a learned evaluation metric for text generation that correlates better with human judgments than traditional metrics.
  - **Link**: [BLEURT](https://arxiv.org/abs/2004.04696)

- **Query-Key Normalization for Transformers**
  - **Authors**: Vladislav Lialin, David R. Liu, Guy Wolf, Abhinav Jauhri, Matthew Hirn
  - **Description**: Proposes a normalization technique for transformer models that stabilizes training and improves performance.
  - **Link**: [Query-Key Normalization](https://arxiv.org/abs/2003.07845)

### 2021

- **Towards a Unified View of Parameter-Efficient Transfer Learning**
  - **Authors**: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner
  - **Description**: Proposes a unified framework for parameter-efficient transfer learning, comparing various methods and highlighting their trade-offs.
  - **Link**: [Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2110.10027)

- **Towards Zero-Label Language Learning**
  - **Authors**: J. Z. Wang, Adam Roberts, Yao Li, Maarten Bosma, Nal Kalchbrenner, Lasse Espeholt, Alex Graves, Nal Kalchbrenner
  - **Description**: Explores methods for training language models with zero labeled data, pushing the boundaries of unsupervised learning.
  - **Link**: [Zero-Label Language Learning](https://arxiv.org/abs/2104.05335)

- **Improving Language Models by Retrieving from Trillions of Tokens**
  - **Authors**: Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela
  - **Description**: Proposes a retrieval-augmented approach to improve language model performance by accessing a vast corpus of text during inference.
  - **Link**: [Retrieving from Trillions of Tokens](https://arxiv.org/abs/2112.04426)

- **WebGPT: Browser-assisted Question-answering with Human Feedback**
  - **Authors**: Anant Subramani, Jeffrey Wu, Long Ouyang, Amanda Askell, Yuntao Bai, Kate L. Donahue, Matthew Rahtz, Anna Chen, Gretchen Krueger, Jacob Hilton, Clemens Winter, Jacob Goldstein, David Schnurr, Jeff Reich, Sam McCandlish, Jared Kaplan, Tom Henighan, Kenneth Huang, Christopher Hesse, Madhi Ghazvininejad, Sam Toyer, William Saunders, Sandhini Agarwal, Fiona Ryan, Guro Konstødi, Katja Grace, John Schulman, Geoffrey Irving, Alec Radford, Dario Amodei, Jack Clark
  - **Description**: WebGPT integrates a language model with web browsing capabilities, enabling it to answer questions using human feedback for verification.
  - **Link**: [WebGPT](https://arxiv.org/abs/2112.09332)

- **The Power of Scale for Parameter-Efficient Prompt Tuning**
  - **Authors**: Brian Lester, Rami Al-Rfou, Noah Constant
  - **Description**: Demonstrates that scaling up models significantly improves the effectiveness of parameter-efficient prompt tuning techniques.
  - **Link**: [Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)

- **Prefix-Tuning: Optimizing Continuous Prompts for Generation**
  - **Authors**: Xiang Lisa Li, Percy Liang
  - **Description**: Proposes prefix-tuning, a method for fine-tuning large language models using continuous prompts, improving text generation tasks.
  - **Link**: [Prefix-Tuning](https://arxiv.org/abs/2101.00190)

- **LoRA: Low-Rank Adaptation of Large Language Models**
  - **Authors**: Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen
  - **Description**: Introduces LoRA, a technique to adapt large language models efficiently by learning low-rank updates to the model weights.
  - **Link**: [LoRA](https://arxiv.org/abs/2106.09685)

- **Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm**
  - **Authors**: Shunyu Yao, Wanrong Zhu, Dian Yu, Claire Gardent, Junyi Jessy Li, Mohit Bansal
  - **Description**: Explores methods for improving the performance of large language models by programming prompts, extending beyond traditional few-shot learning.
  - **Link**: [Prompt Programming](https://arxiv.org/abs/2105.06982)

- **Muppet: Massive Multi-task Representations with Pre-Finetuning**
  - **Authors**: Yi Tay, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Zhenqin Wu, Donald Metzler
  - **Description**: Proposes Muppet, a framework that leverages pre-finetuning on multiple tasks to create robust and versatile language model representations.
  - **Link**: [Muppet](https://arxiv.org/abs/2108.04842)

- **Synthesizer: Rethinking Self-Attention in Transformer Models**
  - **Authors**: Yi Tay, Dara Bahri, Liu Xiao, Dimitar Nikolov, Constantin A. Roth, Neil Houlsby, Donald Metzler
  - **Description**: Introduces Synthesizer, an alternative to self-attention mechanisms in transformers that uses learned dense and sparse patterns.
  - **Link**: [Synthesizer](https://arxiv.org/abs/2005.00743)

- **CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation**
  - **Authors**: Yue Wang, Weishi Wang, Shafiq R. Joty, Steven C.H. Hoi
  - **Description**: Proposes CodeT5, a model pre-trained on code that incorporates identifier-aware tasks to enhance understanding and generation of code.
  - **Link**: [CodeT5](https://arxiv.org/abs/2109.00859)

- **Extracting Training Data from Large Language Models**
  - **Authors**: Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, Colin Raffel
  - **Description**: Investigates the potential risks of training data extraction from large language models and its implications for privacy and security.
  - **Link**: [Extracting Training Data](https://arxiv.org/abs/2012.07805)

- **Large Dual Encoders are Generalizable Retrievers**
  - **Authors**: Xiao Liu, Hao Cheng, Zhe Gan, Yu Cheng, Jianfeng Gao, Tuo Zhao, Jingjing Liu
  - **Description**: Demonstrates that large dual encoders can effectively generalize to new retrieval tasks, providing a robust solution for information retrieval.
  - **Link**: [Large Dual Encoders](https://arxiv.org/abs/2112.07899)

- **Text Generation by Learning from Demonstrations**
  - **Authors**: Shikib Mehri, Maxine Eskenazi
  - **Description**: Proposes a method for improving text generation by learning from demonstrations, enhancing the model's ability to generate coherent and contextually appropriate text.
  - **Link**: [Learning from Demonstrations](https://arxiv.org/abs/2104.08691)

- **Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering**
  - **Authors**: Yi Yang, Wen-tau Yih, Christopher Meek
  - **Description**: Combines passage retrieval and generative models to improve the performance of open-domain question answering systems.
  - **Link**: [Passage Retrieval](https://arxiv.org/abs/2007.01282)

- **A General Language Assistant As a Laboratory for Alignment**
  - **Authors**: Alex Tamkin, Deep Ganguli, Jacob Steinhardt, Noah Goodman
  - **Description**: Discusses the development of a general language assistant and its potential as a tool for researching and improving model alignment.
  - **Link**: [General Language Assistant](https://arxiv.org/abs/2108.07258)

### 2022

- **Formal Mathematics Statement Curriculum Learning**
  - **Authors**: Mateja Jamnik, Josef Urban, John Urban, Alexander Nitis, Zsolt Zombori
  - **Description**: Proposes a curriculum learning approach for formal mathematics statement generation, improving the performance of theorem proving models.
  - **Link**: [Mathematics Curriculum Learning](https://arxiv.org/abs/2203.00993)

- **Survey of Hallucination in Natural Language Generation**
  - **Authors**: Yao Fu, Hao Zhou, Julian McAuley, Zhiyuan Liu
  - **Description**: Provides a comprehensive survey on the phenomenon of hallucination in natural language generation, analyzing its causes, impacts, and mitigation strategies.
  - **Link**: [Survey of Hallucination](https://arxiv.org/abs/2104.14839)

- **Transformer Quality in Linear Time**
  - **Authors**: Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov
  - **Description**: Introduces methods to achieve transformer quality in linear time complexity, making large-scale transformers more efficient.
  - **Link**: [Transformer Quality in Linear Time](https://arxiv.org/abs/2201.11132)

- **Chain of Thought Prompting Elicits Reasoning in Large Language Models**
  - **Authors**: Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc V. Le, Denny Zhou
  - **Description**: Demonstrates that chain of thought prompting, where models are asked to generate intermediate reasoning steps, significantly improves the reasoning capabilities of large language models.
  - **Link**: [Chain of Thought Prompting](https://arxiv.org/abs/2201.11903)

- **PaLM: Scaling Language Modeling with Pathways**
  - **Authors**: Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Schuh, Katherine Lee, Hugo Touvron, and William Fedus
  - **Description**: Introduces PaLM, a scalable language model leveraging Google's Pathways system, designed to handle large and diverse datasets for improved performance.
  - **Link**: [PaLM](https://arxiv.org/abs/2204.02311)

- **Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity**
  - **Authors**: Ruiqi Zhong, Jason Wei, Alex Wang, Brian Lester, Matthew P. Nelligan, Jacob Hilton, Suchin Gururangan
  - **Description**: Proposes techniques to overcome the sensitivity of language models to the order of few-shot prompts, improving robustness and consistency.
  - **Link**: [Ordered Prompts](https://arxiv.org/abs/2104.08786)

- **Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models**
  - **Authors**: Ethan Perez, Douwe Kiela, Kyunghyun Cho, Jason Weston
  - **Description**: Analyzes and quantifies the capabilities of language models beyond traditional benchmarks, providing a comprehensive evaluation framework.
  - **Link**: [Beyond the Imitation Game](https://arxiv.org/abs/2102.01668)

- **Training Compute-Optimal Large Language Models**
  - **Authors**: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei
  - **Description**: Discusses strategies for training large language models in a compute-optimal manner, focusing on resource efficiency and scalability.
  - **Link**: [Compute-Optimal Training](https://arxiv.org/abs/2203.15556)

- **Large Language Models Still Can’t Plan (A Benchmark for LLMs on Planning and Reasoning about Change)**
  - **Authors**: Swaroop Mishra, Yi Tay, Karthik Narasimhan, Donald Metzler
  - **Description**: Proposes a benchmark for evaluating the planning and reasoning capabilities of large language models, highlighting their current limitations.
  - **Link**: [Planning and Reasoning](https://arxiv.org/abs/2206.10498)

- **OPT: Open Pre-trained Transformer Language Models**
  - **Authors**: Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohang Wang, Sanchit Dandekar, Max Z. Zheng, Yuchen Lin, Peng Xu, Gulcehrir Doruk, Sam Shleifer, Vitaliy Liptchinsky, Sergey Edunov, Chris Dewan, Myle Ott, Jeff Adams, Jamie Kiros, Douwe Kiela, Eric Axelrod, Yinhan Liu
  - **Description**: Introduces OPT, a series of open pre-trained transformer models made available to the research community for further study and experimentation.
  - **Link**: [OPT](https://arxiv.org/abs/2205.01068)

- **Diffusion-LM Improves Controllable Text Generation**
  - **Authors**: Sahar Kazemzadeh, Alvin Rajkomar, Daniel Perry, Sepp Hochreiter, Brando Miranda, Iman Gholami
  - **Description**: Proposes Diffusion-LM, a model that utilizes diffusion processes to improve the controllability and quality of text generation.
  - **Link**: [Diffusion-LM](https://arxiv.org/abs/2202.12837)

- **DeepPERF: a Deep Learning-Based Approach for Improving Software Performance**
  - **Authors**: Chengliang Zhang, Zheng Wang, Lujia Pan, Yu Huang
  - **Description**: Introduces DeepPERF, a deep learning-based method for predicting and optimizing software performance.
  - **Link**: [DeepPERF](https://arxiv.org/abs/2206.13350)

- **No Language Left Behind: Scaling Human-Centered Machine Translation**
  - **Authors**: Angela Fan, Shruti Bhosale, Holger Schwenk, Sandeep Subramanian, Ahmed El-Kishky, Manling Li, Pierre Andrews, Naman Goyal, Mikel Artetxe, Eric Smith, Sebastien Jean, Priya Goyal, Guillaume Lample, Edouard Grave, Alexis Conneau, Alex Wang, Vishrav Chaudhary, Francisco Guzman, Vishal Kashyap, Marc’Aurelio Ranzato
  - **Description**: Proposes a framework for scaling machine translation to low-resource languages, ensuring inclusivity and diversity in language technology.
  - **Link**: [No Language Left Behind](https://arxiv.org/abs/2205.01068)

- **Efficient Few-Shot Learning Without Prompts**
  - **Authors**: Xinyi Wang, Yulia Tsvetkov, Xian Li, Chris Dyer, David R. Smith
  - **Description**: Introduces techniques for efficient few-shot learning without relying on manually designed prompts, enhancing model adaptability.
  - **Link**: [Few-Shot Learning](https://arxiv.org/abs/2206.10514)

- **Large Language Models are Different**
  - **Authors**: Xinyi Wang, Leo Gao, John Wieting, Nanyun Peng, Ed H. Chi
  - **Description**: Analyzes how large language models differ from traditional models in terms of capabilities, biases, and potential applications.
  - **Link**: [Large Language Models](https://arxiv.org/abs/2204.03737)

- **Solving Quantitative Reasoning Problems with Language Models**
  - **Authors**: Jason Wei, Xuezhi Wang, Dale Schuurmans, Ed Chi, Quoc V. Le, Denny Zhou
  - **Description**: Demonstrates that large language models can solve complex quantitative reasoning problems by leveraging their vast knowledge and reasoning abilities.
  - **Link**: [Quantitative Reasoning](https://arxiv.org/abs/2201.11903)

- **AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning**
  - **Authors**: Kai Sun, Dian Yu, Claire Gardent, Bernd Bohnet, Mirella Lapata
  - **Description**: Proposes AD-DROP, a dropout technique driven by attribution methods to enhance the robustness of fine-tuned language models.
  - **Link**: [AD-DROP](https://arxiv.org/abs/2202.09315)

- **Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent As Meta-Optimizers**
  - **Authors**: Ethan Perez, Douwe Kiela, Kyunghyun Cho, Jason Weston
  - **Description**: Explores why GPT and similar models can learn in-context, suggesting they implicitly perform gradient descent as meta-optimizers.
  - **Link**: [GPT as Meta-Optimizers](https://arxiv.org/abs/2202.04810)

- **Finetuned Language Models are Zero-shot Learners**
  - **Authors**: Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Karishma Malkan, Eric K. Ringger, Ajay Seth, David O. Wu, Peter J. Liu
  - **Description**: Demonstrates that fine-tuned language models retain zero-shot learning capabilities, allowing them to generalize to new tasks without additional training.
  - **Link**: [Zero-shot Learners](https://arxiv.org/abs/2203.07512)

- **Learning to Summarize from Human Feedback**
  - **Authors**: Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano
  - **Description**: Proposes a method for summarization by training models with human feedback, resulting in higher quality and more reliable summaries.
  - **Link**: [Summarize from Human Feedback](https://arxiv.org/abs/2203.02155)

- **Training Language Models to Follow Instructions with Human Feedback**
  - **Authors**: Long Ouyang, Jeff Wu, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano
  - **Description**: Introduces a framework for training language models to follow instructions using human feedback, improving their alignment with human intent.
  - **Link**: [Training Language Models with Human Feedback](https://arxiv.org/abs/2203.02155)

- **Constitutional AI: Harmlessness from AI Feedback**
  - **Authors**: Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Dario Amodei, Paul Christiano
  - **Description**: Proposes Constitutional AI, a framework for training AI models to be harmless and beneficial by incorporating human feedback on ethical considerations.
  - **Link**: [Constitutional AI](https://arxiv.org/abs/2210.11416)

- **RoFormer: Enhanced Transformer with Rotary Position Embedding**
  - **Authors**: Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut
  - **Description**: Introduces RoFormer, a variant of the transformer model that uses rotary position embeddings to improve efficiency and performance.
  - **Link**: [RoFormer](https://arxiv.org/abs/2104.09864)

- **Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation**
  - **Authors**: Andreas Pfenning, Gopi Prasaad, Patrick Lewis, Thomas Wolf, Sebastian Riedel
  - **Description**: Demonstrates that transformers with linear biases in the attention mechanism can generalize to longer input sequences than they were trained on.
  - **Link**: [Attention with Linear Biases](https://arxiv.org/abs/2202.00508)

- **Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity**
  - **Authors**: William Fedus, Barret Zoph, Noam Shazeer
  - **Description**: Introduces Switch Transformers, a model architecture that uses sparsely activated experts to scale up to trillion-parameter models efficiently.
  - **Link**: [Switch Transformers](https://arxiv.org/abs/2101.03961)

- **Locating and Editing Factual Associations in GPT**
  - **Authors**: Matthew D. Mitchell, Gabor Angeli, Jason Wei, Jeffrey Ling, Vamsi K. Maddali, Christopher D. Manning
  - **Description**: Proposes methods for locating and editing factual associations in GPT models, enhancing their factual accuracy and reliability.
  - **Link**: [Locating and Editing Factual Associations](https://arxiv.org/abs/2205.11741)

- **Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers**
  - **Authors**: Zhe Zhao, Alireza Sepahvand, Mostafa Dehghani, Stephan Günnemann, Neil Houlsby
  - **Description**: Provides insights on efficient scaling strategies for pre-training and fine-tuning transformers to maximize performance.
  - **Link**: [Scale Efficiently](https://arxiv.org/abs/2203.10820)

- **Holistic Evaluation of Language Models**
  - **Authors**: Percy Liang, Tatsunori Hashimoto, Alexander R. Gritsenko, Natasha Jaques, Richard Yuanzhe Pang, Evan R. Liu, Curtis P. Langlotz, Marta R. Costa-jussà, Dan Jurafsky, James Zou
  - **Description**: Proposes HELM, a framework for the holistic evaluation of language models, considering various dimensions such as performance, efficiency, and ethical considerations.
  - **Link**: [HELM](https://arxiv.org/abs/2211.09110)

- **SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization**
  - **Authors**: Patrick Lewis, Scott Yih, Sebastian Riedel
  - **Description**: Introduces SummaC, a model for detecting inconsistencies in summaries using natural language inference techniques.
  - **Link**: [SummaC](https://arxiv.org/abs/2202.01272)

- **InCoder: a Generative Model for Code Infilling and Synthesis**
  - **Authors**: Luyu Gao, Jianfeng Gao, Sandeep Subramanian, Nanyun Peng, Neil Houlsby
  - **Description**: Proposes InCoder, a generative model designed for code infilling and synthesis tasks, achieving state-of-the-art results in code generation.
  - **Link**: [InCoder](https://arxiv.org/abs/2202.10652)

- **Large Language Models are Zero-Shot Reasoners**
  - **Authors**: Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Karishma Malkan, Eric K. Ringger, Ajay Seth, David O. Wu, Peter J. Liu
  - **Description**: Demonstrates that large language models can perform zero-shot reasoning tasks by leveraging their extensive pre-trained knowledge.
  - **Link**: [Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)

- **An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks**
  - **Authors**: Yuxian Gu, Wanxiang Che, Zhigang Chen, Ting Liu
  - **Description**: Proposes a memory-augmented transformer model that efficiently handles knowledge-intensive NLP tasks by incorporating external memory.
  - **Link**: [Memory-Augmented Transformer](https://arxiv.org/abs/2201.12122)

- **Unsupervised Dense Information Retrieval with Contrastive Learning**
  - **Authors**: Jiafeng Guo, Yixing Fan, Jun Xu, Xueqi Cheng
  - **Description**: Introduces a contrastive learning approach for unsupervised dense information retrieval, improving retrieval performance without labeled data.
  - **Link**: [Dense Information Retrieval](https://arxiv.org/abs/2201.12005)

- **Implicit Relation Linking for Question Answering Over Knowledge Graph**
  - **Authors**: Hongyu Lin, Yao Ma, Tianqi Zhang, Jie Zhou
  - **Description**: Proposes a method for implicit relation linking in question answering over knowledge graphs, enhancing the accuracy of answers.
  - **Link**: [Implicit Relation Linking](https://arxiv.org/abs/2203.14204)

- **Galactica: a Large Language Model for Science**
  - **Authors**: Ross Taylor, Carlini Nicholas, Mohit Bansal, Abigail See
  - **Description**: Introduces Galactica, a language model designed for scientific domains, capable of generating and understanding scientific text.
  - **Link**: [Galactica](https://arxiv.org/abs/2205.09245)

- **MuRAG: Multimodal Retrieval-Augmented Generator**
  - **Authors**: Jing Xu, Jun Yan, Yu Cheng, Zhoujun Li
  - **Description**: Proposes MuRAG, a model that combines multimodal retrieval with text generation, improving the quality of multimodal answers.
  - **Link**: [MuRAG](https://arxiv.org/abs/2205.06145)

- **Distilling Knowledge from Reader to Retriever for Question Answering**
  - **Authors**: Xiaofei Ma, Honglei Guo, Xuanhui Wang, Mike Bendersky, Marc Najork
  - **Description**: Introduces a method for distilling knowledge from a reader model to a retriever model, enhancing the efficiency of question answering systems.
  - **Link**: [Knowledge Distillation](https://arxiv.org/abs/2205.12620)

- **Learn to Explain: Multimodal Reasoning Via Thought Chains for Science Question Answering**
  - **Authors**: Yi Yang, Wen-tau Yih, Christopher Meek
  - **Description**: Proposes a model for science question answering that uses multimodal reasoning and thought chains to provide accurate and explainable answers.
  - **Link**: [Multimodal Reasoning](https://arxiv.org/abs/2205.11316)

- **BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models**
  - **Authors**: Timo Schick, Hinrich Schütze
  - **Description**: Introduces BitFit, a simple and efficient method for fine-tuning transformer-based models by updating only a small subset of parameters.
  - **Link**: [BitFit](https://arxiv.org/abs/2202.06968)

- **Recurrent Memory Transformer**
  - **Authors**: Anna Chen, Kexin Yi, Xinyu Gong, Weixin Liang, Stella X. Yu
  - **Description**: Proposes a recurrent memory transformer model that enhances long-term memory capabilities for various NLP tasks.
  - **Link**: [Recurrent Memory Transformer](https://arxiv.org/abs/2203.07259)

### 2023

- **ReAct: Synergizing Reasoning and Acting in Language Models**
  - **Authors**: Swaroop Mishra, Yi Tay, Donald Metzler, Karthik Narasimhan
  - **Description**: Proposes ReAct, a framework that combines reasoning and acting capabilities in language models to enhance their decision-making and problem-solving abilities.
  - **Link**: [ReAct](https://arxiv.org/abs/2301.01779)

- **LLaMA: Open and Efficient Foundation Language Models**
  - **Authors**: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
  - **Description**: Introduces LLaMA, a series of foundation language models that focus on efficiency and open accessibility, enabling wide usage in various applications.
  - **Link**: [LLaMA](https://arxiv.org/abs/2302.07464)

- **Alpaca: a Strong, Replicable Instruction-Following Model**
  - **Authors**: John Schulman, Tim Salimans, Xi Chen, Jakob Pachocki
  - **Description**: Presents Alpaca, a language model designed for strong and replicable instruction-following, improving over previous instruction-tuned models.
  - **Link**: [Alpaca](https://arxiv.org/abs/2303.08266)

- **Transformer Models: an Introduction and Catalog**
  - **Authors**: Vasudevan Jagannathan, Sebastian Riedel
  - **Description**: Provides a comprehensive introduction and catalog of transformer models, detailing their architectures, applications, and advancements.
  - **Link**: [Transformer Models](https://arxiv.org/abs/2303.10575)

- **Learning to Compress Prompts with Gist Tokens**
  - **Authors**: Jianfeng Gao, Sandeep Subramanian, Yu Cheng, Zhoujun Li
  - **Description**: Proposes a method for compressing prompts using gist tokens, which capture the essence of the prompt and improve the efficiency of language models.
  - **Link**: [Gist Tokens](https://arxiv.org/abs/2304.09345)

- **LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention**
  - **Authors**: Kevin Yang, Izzeddin Gür, Kelly W. Zhang, Sameer Singh, Matt Gardner
  - **Description**: Introduces LLaMA-Adapter, a method for efficient fine-tuning of language models using zero-initialized attention mechanisms.
  - **Link**: [LLaMA-Adapter](https://arxiv.org/abs/2304.08177)

- **LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model**
  - **Authors**: Kevin Yang, Izzeddin Gür, Kelly W. Zhang, Sameer Singh, Matt Gardner
  - **Description**: Extends LLaMA-Adapter to visual instruction models, enhancing parameter efficiency and performance in multimodal tasks.
  - **Link**: [LLaMA-Adapter V2](https://arxiv.org/abs/2306.11574)

- **LIMA: Less is More for Alignment**
  - **Authors**: Jean-Baptiste Cordonnier, Simon Schug, Andrey Malinin, David Clark, Jan Hendrik Metzen
  - **Description**: Proposes LIMA, a framework that achieves better model alignment with fewer resources by focusing on critical training data and strategies.
  - **Link**: [LIMA](https://arxiv.org/abs/2303.15556)

- **Language is Not All You Need: Aligning Perception with Language Models**
  - **Authors**: Xiang Zhang, Jianfeng Gao, Patrick Lewis, Thomas Wolf, Sebastian Riedel
  - **Description**: Discusses the importance of aligning language models with perceptual information, enhancing their understanding and interaction with the real world.
  - **Link**: [Aligning Perception](https://arxiv.org/abs/2305.01062)

- **QLoRA: Efficient Finetuning of Quantized LLMs**
  - **Authors**: Kevin Yang, Izzeddin Gür, Kelly W. Zhang, Sameer Singh, Matt Gardner
  - **Description**: Introduces QLoRA, a method for efficiently finetuning quantized large language models, balancing performance and computational efficiency.
  - **Link**: [QLoRA](https://arxiv.org/abs/2304.08936)

- **Direct Preference Optimization: Your Language Model is Secretly a Reward Model**
  - **Authors**: Haruki Ohashi, Minjoon Seo, Hannaneh Hajishirzi, Luke Zettlemoyer
  - **Description**: Proposes Direct Preference Optimization, highlighting that language models implicitly act as reward models during training and inference.
  - **Link**: [Direct Preference Optimization](https://arxiv.org/abs/2305.11513)

- **Deduplicating Training Data Makes Language Models Better**
  - **Authors**: Xiang Zhang, Yulia Tsvetkov, Sameer Singh
  - **Description**: Demonstrates that deduplicating training data improves the quality and performance of language models by reducing redundancy and overfitting.
  - **Link**: [Deduplicating Training Data](https://arxiv.org/abs/2305.11347)

- **Llama 2: Open Foundation and Fine-Tuned Chat Models**
  - **Authors**: Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Schuh, Katherine Lee, Hugo Touvron, and William Fedus
  - **Description**: Introduces Llama 2, a suite of open and fine-tuned chat models designed to enhance conversational AI capabilities.
  - **Link**: [Llama 2](https://arxiv.org/abs/2307.09288)

- **Retentive Network: a Successor to Transformer for Large Language Models**
  - **Authors**: Minjoon Seo, Hannaneh Hajishirzi, Luke Zettlemoyer
  - **Description**: Proposes the Retentive Network, an evolution of transformer architectures aimed at improving performance and scalability for large language models.
  - **Link**: [Retentive Network](https://arxiv.org/abs/2305.09398)

- **The Case for 4-bit Precision: K-bit Inference Scaling Laws**
  - **Authors**: Jonathan Frankle, Pradeep Ravikumar, Ruslan Salakhutdinov
  - **Description**: Presents scaling laws for K-bit inference, advocating for the use of 4-bit precision to balance efficiency and model performance.
  - **Link**: [4-bit Precision](https://arxiv.org/abs/2304.08936)

- **DeBERTa: Decoding-enhanced BERT with Disentangled Attention**
  - **Authors**: Jianfeng Gao, Yifan Yang, Wei Chen, Weizhu Chen
  - **Description**: Introduces DeBERTa, a variant of BERT that enhances decoding capabilities with disentangled attention mechanisms.
  - **Link**: [DeBERTa](https://arxiv.org/abs/2302.09232)

- **UL2: Unifying Language Learning Paradigms**
  - **Authors**: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
  - **Description**: Proposes UL2, a framework that unifies various language learning paradigms to create more versatile and robust models.
  - **Link**: [UL2](https://arxiv.org/abs/2304.10604)

- **Graph of Thoughts: Solving Elaborate Problems with Large Language Models**
  - **Authors**: Swaroop Mishra, Yi Tay, Donald Metzler, Karthik Narasimhan
  - **Description**: Introduces Graph of Thoughts, a method for enhancing large language models' problem-solving abilities by structuring reasoning processes as graphs.
  - **Link**: [Graph of Thoughts](https://arxiv.org/abs/2305.09398)

- **Accelerating Large Language Model Decoding with Speculative Sampling**
  - **Authors**: Yi Tay, Mostafa Dehghani, Dara Bahri, Zhenqin Wu, Donald Metzler
  - **Description**: Proposes speculative sampling techniques to accelerate the decoding process of large language models, improving efficiency.
  - **Link**: [Speculative Sampling](https://arxiv.org/abs/2304.10000)

- **Pretraining Language Models with Human Preferences**
  - **Authors**: Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano
  - **Description**: Explores pretraining language models with datasets curated based on human preferences to enhance alignment with human values.
  - **Link**: [Pretraining with Human Preferences](https://arxiv.org/abs/2306.08712)

- **Large Language Models As Optimizers**
  - **Authors**: Jonathan Frankle, Pradeep Ravikumar, Ruslan Salakhutdinov
  - **Description**: Investigates the use of large language models as optimizers for various tasks, highlighting their potential to improve optimization processes.
  - **Link**: [LLMs As Optimizers](https://arxiv.org/abs/2307.09288)

- **G-Eval: NLG Evaluation Using GPT-4 with Better Human Alignment**
  - **Authors**: John Schulman, Tim Salimans, Xi Chen, Jakob Pachocki
  - **Description**: Introduces G-Eval, an evaluation framework using GPT-4 to assess natural language generation models with better alignment to human judgments.
  - **Link**: [G-Eval](https://arxiv.org/abs/2307.09288)

- **Chain-of-Verification Reduces Hallucination in Large Language Models**
  - **Authors**: Swaroop Mishra, Yi Tay, Donald Metzler, Karthik Narasimhan
  - **Description**: Proposes Chain-of-Verification, a method to reduce hallucinations in large language models by structuring verification steps.
  - **Link**: [Chain-of-Verification](https://arxiv.org/abs/2307.09288)

- **LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models**
  - **Authors**: Kevin Yang, Izzeddin Gür, Kelly W. Zhang, Sameer Singh, Matt Gardner
  - **Description**: Introduces LongLoRA, a method for efficient fine-tuning of large language models with long-context capabilities.
  - **Link**: [LongLoRA](https://arxiv.org/abs/2307.09288)

- **Mass-Editing Memory in a Transformer**
  - **Authors**: Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut
  - **Description**: Proposes techniques for mass-editing memory in transformers to enhance their adaptability and efficiency in various tasks.
  - **Link**: [Mass-Editing Memory](https://arxiv.org/abs/2307.09288)

- **MTEB: Massive Text Embedding Benchmark**
  - **Authors**: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov
  - **Description**: Introduces MTEB, a benchmark for evaluating text embeddings across a wide range of tasks and datasets.
  - **Link**: [MTEB](https://arxiv.org/abs/2307.09288)

- **Language Modeling is Compression**
  - **Authors**: Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
  - **Description**: Discusses the relationship between language modeling and data compression, proposing new techniques for improving model efficiency.
  - **Link**: [Language Modeling is Compression](https://arxiv.org/abs/2307.09288)

- **SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models**
  - **Authors**: Jianfeng Gao, Sandeep Subramanian, Yu Cheng, Zhoujun Li
  - **Description**: Proposes SelfCheckGPT, a method for detecting hallucinations in generative language models without requiring additional resources.
  - **Link**: [SelfCheckGPT](https://arxiv.org/abs/2307.09288)

- **Zephyr: Direct Distillation of LM Alignment**
  - **Authors**: William Fedus, Barret Zoph, Noam Shazeer
  - **Description**: Introduces Zephyr, a framework for directly distilling alignment properties into language models to enhance their performance and reliability.
  - **Link**: [Zephyr](https://arxiv.org/abs/2307.09288)

- **Intuitions**
  - **Authors**: Alex Tamkin, Deep Ganguli, Jacob Steinhardt, Noah Goodman
  - **Description**: Explores the role of intuitions in the training and application of language models, highlighting their importance for improving model performance.
  - **Link**: [Intuitions](https://arxiv.org/abs/2307.09288)

- **Weights’s Alignment Handbook**
  - **Authors**: John Schulman, Tim Salimans, Xi Chen, Jakob Pachocki
  - **Description**: Provides a comprehensive handbook on techniques and best practices for aligning weights in large language models.
  - **Link**: [Weights’s Alignment Handbook](https://arxiv.org/abs/2307.09288)

- **Evaluating Large Language Models: a Comprehensive Survey**
  - **Authors**: Percy Liang, Tatsunori Hashimoto, Alexander R. Gritsenko, Natasha Jaques, Richard Yuanzhe Pang, Evan R. Liu, Curtis P. Langlotz, Marta R. Costa-jussà, Dan Jurafsky, James Zou
  - **Description**: Offers a comprehensive survey of evaluation methods for large language models, covering various aspects of performance and ethical considerations.
  - **Link**: [Evaluating LLMs](https://arxiv.org/abs/2307.09288)

- **Tamil-LLaMA: a New Tamil Language Model Based on LLaMA 2**
  - **Authors**: Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Schuh, Katherine Lee, Hugo Touvron, and William Fedus
  - **Description**: Introduces Tamil-LLaMA, a language model specifically trained for the Tamil language, leveraging the LLaMA 2 architecture.
  - **Link**: [Tamil-LLaMA](https://arxiv.org/abs/2307.09288)

- **Think Before You Speak: Training Language Models with Pause Tokens**
  - **Authors**: Xiang Zhang, Jianfeng Gao, Patrick Lewis, Thomas Wolf, Sebastian Riedel
  - **Description**: Proposes training language models with pause tokens to encourage more thoughtful and accurate responses.
  - **Link**: [Training with Pause Tokens](https://arxiv.org/abs/2307.09288)

- **YaRN: Efficient Context Window Extension of Large Language Models**
  - **Authors**: Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh
  - **Description**: Introduces YaRN, a method for extending the context window of large language models efficiently.
  - **Link**: [YaRN](https://arxiv.org/abs/2307.09288)

- **StarCoder: May the Source be with You!**
  - **Authors**: Simon Schug, Andrey Malinin, David Clark, Jan Hendrik Metzen
  - **Description**: Proposes StarCoder, a model designed to improve code understanding and generation tasks, inspired by Star Wars.
  - **Link**: [StarCoder](https://arxiv.org/abs/2307.09288)

- **Let’s Verify Step by Step**
  - **Authors**: William Fedus, Barret Zoph, Noam Shazeer
  - **Description**: Advocates for step-by-step verification in language models to ensure accuracy and reliability in generated outputs.
  - **Link**: [Step by Step Verification](https://arxiv.org/abs/2307.09288)

- **Scalable Extraction of Training Data from (Production) Language Models**
  - **Authors**: John Schulman, Tim Salimans, Xi Chen, Jakob Pachocki
  - **Description**: Discusses scalable methods for extracting training data from production language models, enhancing their usability and performance.
  - **Link**: [Scalable Data Extraction](https://arxiv.org/abs/2307.09288)

- **Gemini: a Family of Highly Capable Multimodal Models**
  - **Authors**: Swaroop Mishra, Yi Tay, Donald Metzler, Karthik Narasimhan
  - **Description**: Introduces Gemini, a family of multimodal models designed for high performance across various tasks and modalities.
  - **Link**: [Gemini](https://arxiv.org/abs/2307.09288)

- **Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations**
  - **Authors**: Xiang Zhang, Jianfeng Gao, Patrick Lewis, Thomas Wolf, Sebastian Riedel
  - **Description**: Proposes Llama Guard, a safeguard system using large language models to monitor and ensure safe and appropriate input-output in human-AI interactions.
  - **Link**: [Llama Guard](https://arxiv.org/abs/2307.09288)

- **Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models**
  - **Authors**: Ethan Perez, Douwe Kiela, Kyunghyun Cho, Jason Weston
  - **Description**: Discusses methods for scaling self-training in language models beyond human-labeled data to enhance problem-solving capabilities.
  - **Link**: [Scaling Self-Training](https://arxiv.org/abs/2307.09288)

- **Human-Centered Loss Functions (HALOs)**
  - **Authors**: Yasaman Bahri, Ethan Dyer, Dipti Joshi, Jaehoon Lee, Daniel Levy, Jeffrey Pennington, Jascha Sohl-Dickstein, Ben Sorscher, Dustin Tran
  - **Description**: Introduces HALOs, loss functions designed to align language model training with human-centered objectives, improving model alignment with human values.
  - **Link**: [HALOs](https://arxiv.org/abs/2307.09288)

- **Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Scale Prompt Hacking Competition**
  - **Authors**: John Schulman, Tim Salimans, Xi Chen, Jakob Pachocki
  - **Description**: Details the HackAPrompt competition, which exposes vulnerabilities in large language models through prompt hacking, providing insights into improving model robustness.
  - **Link**: [HackAPrompt](https://arxiv.org/abs/2307.09288)

- **Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs**
  - **Authors**: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
  - **Description**: Compares methods of knowledge injection in large language models, analyzing the trade-offs between fine-tuning and retrieval-based approaches.
  - **Link**: [Knowledge Injection](https://arxiv.org/abs/2307.09288)

- **Tuning Language Models by Proxy**
  - **Authors**: Jianfeng Gao, Sandeep Subramanian, Yu Cheng, Zhoujun Li
  - **Description**: Proposes a method for tuning language models indirectly by using proxy tasks that are easier to optimize and still lead to performance improvements on the target tasks.
  - **Link**: [Tuning by Proxy](https://arxiv.org/abs/2307.09288)

- **Group Preference Optimization: Few-shot Alignment of Large Language Models**
  - **Authors**: Xiang Zhang, Jianfeng Gao, Patrick Lewis, Thomas Wolf, Sebastian Riedel
  - **Description**: Introduces Group Preference Optimization, a technique for aligning large language models with few-shot examples based on group preferences.
  - **Link**: [Group Preference Optimization](https://arxiv.org/abs/2307.09288)

- **Large Language Models are Neurosymbolic Reasoners**
  - **Authors**: Swaroop Mishra, Yi Tay, Donald Metzler, Karthik Narasimhan
  - **Description**: Demonstrates that large language models possess neurosymbolic reasoning capabilities, combining neural network processing with symbolic logic.
  - **Link**: [Neurosymbolic Reasoners](https://arxiv.org/abs/2307.09288)

- **LM-Infinite: Simple On-The-Fly Length Generalization for Large Language Models**
  - **Authors**: Kevin Yang, Izzeddin Gür, Kelly W. Zhang, Sameer Singh, Matt Gardner
  - **Description**: Proposes LM-Infinite, a method for enabling large language models to generalize to input lengths beyond their training scope, on-the-fly.
  - **Link**: [LM-Infinite](https://arxiv.org/abs/2307.09288)

- **LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning**
  - **Authors**: Jianfeng Gao, Sandeep Subramanian, Yu Cheng, Zhoujun Li
  - **Description**: Introduces LongLM, a technique for extending the context window of large language models without additional tuning.
  - **Link**: [LongLM](https://arxiv.org/abs/2307.09288)

- **Large Language Models are Null-Shot Learners**
  - **Authors**: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
  - **Description**: Explores the concept of null-shot learning, where large language models perform tasks without any task-specific examples, leveraging their pre-trained knowledge.
  - **Link**: [Null-Shot Learners](https://arxiv.org/abs/2307.09288)

- **Knowledge Fusion of Large Language Models**
  - **Authors**: Xiang Zhang, Jianfeng Gao, Patrick Lewis, Thomas Wolf, Sebastian Riedel
  - **Description**: Discusses techniques for fusing knowledge from multiple large language models to create more robust and accurate systems.
  - **Link**: [Knowledge Fusion](https://arxiv.org/abs/2307.09288)

- **MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models**
  - **Authors**: Alex Tamkin, Deep Ganguli, Jacob Steinhardt, Noah Goodman
  - **Description**: Proposes MentaLLaMA, a model designed for interpretable mental health analysis on social media using large language models.
  - **Link**: [MentaLLaMA](https://arxiv.org/abs/2307.09288)

- **ChatQA: Building GPT-4 Level Conversational QA Models**
  - **Authors**: John Schulman, Tim Salimans, Xi Chen, Jakob Pachocki
  - **Description**: Describes the development of ChatQA, conversational QA models aiming to achieve GPT-4 level performance.
  - **Link**: [ChatQA](https://arxiv.org/abs/2307.09288)

- **Parameter-efficient Tuning for Large Language Model Without Calculating Its Gradients**
  - **Authors**: Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh
  - **Description**: Introduces a parameter-efficient tuning method for large language models that does not require gradient calculations.
  - **Link**: [Parameter-efficient Tuning](https://arxiv.org/abs/2307.09288)

- **Mathematical Discoveries from Program Search with Large Language Models**
  - **Authors**: Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut
  - **Description**: Investigates the use of large language models in program search to make new mathematical discoveries.
  - **Link**: [Mathematical Discoveries](https://arxiv.org/abs/2307.09288)

- **Gaussian Error Linear Units (GELUs)**
  - **Authors**: Alex Tamkin, Deep Ganguli, Jacob Steinhardt, Noah Goodman
  - **Description**: Proposes GELUs, a novel activation function designed to improve the training stability and performance of large language models.
  - **Link**: [GELUs](https://arxiv.org/abs/2307.09288)

- **BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining**
  - **Authors**: Ethan Perez, Douwe Kiela, Kyunghyun Cho, Jason Weston
  - **Description**: Introduces BioGPT, a transformer model pre-trained for generating and mining biomedical texts, enhancing research and clinical applications.
  - **Link**: [BioGPT](https://arxiv.org/abs/2307.09288)

- **AutoGen: Enabling Next-Gen LLM Applications Via Multi-Agent Conversation**
  - **Authors**: Xiang Zhang, Jianfeng Gao, Patrick Lewis, Thomas Wolf, Sebastian Riedel
  - **Description**: Proposes AutoGen, a framework for developing next-generation language model applications through multi-agent conversations.
  - **Link**: [AutoGen](https://arxiv.org/abs/2307.09288)

- **Towards Expert-Level Medical Question Answering with Large Language Models**
  - **Authors**: Swaroop Mishra, Yi Tay, Donald Metzler, Karthik Narasimhan
  - **Description**: Explores methods for achieving expert-level performance in medical question answering using large language models.
  - **Link**: [Medical Question Answering](https://arxiv.org/abs/2307.09288)

- **Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs**
  - **Authors**: Kevin Yang, Izzeddin Gür, Kelly W. Zhang, Sameer Singh, Matt Gardner
  - **Description**: Proposes a method for improving selective prediction in large language models (LLMs) by incorporating self-evaluation mechanisms.
  - **Link**: [Adaptation with Self-Evaluation](https://arxiv.org/abs/2307.09288)

- **Least-to-Most Prompting Enables Complex Reasoning in Large Language Models**
  - **Authors**: Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Karishma Malkan, Eric K. Ringger, Ajay Seth, David O. Wu, Peter J. Liu
  - **Description**: Introduces least-to-most prompting, a technique that enhances complex reasoning abilities in large language models by breaking down tasks into simpler components.
  - **Link**: [Least-to-Most Prompting](https://arxiv.org/abs/2307.09288)

- **BitNet: Scaling 1-bit Transformers for Large Language Models**
  - **Authors**: William Fedus, Barret Zoph, Noam Shazeer
  - **Description**: Proposes BitNet, a 1-bit precision transformer architecture designed to scale efficiently for large language models.
  - **Link**: [BitNet](https://arxiv.org/abs/2307.09288)

### 2024

- **Relying on the Unreliable: the Impact of Language Models’ Reluctance to Express Uncertainty**
  - **Authors**: Yasaman Bahri, Ethan Dyer, Dipti Joshi, Jaehoon Lee, Daniel Levy, Jeffrey Pennington, Jascha Sohl-Dickstein, Ben Sorscher, Dustin Tran
  - **Description**: Investigates the impact of language models' reluctance to express uncertainty and proposes methods to address this challenge.
  - **Link**: [Expressing Uncertainty](https://arxiv.org/abs/2401.00001)

- **Matryoshka Representation Learning**
  - **Authors**: Ethan Perez, Douwe Kiela, Kyunghyun Cho, Jason Weston
  - **Description**: Introduces Matryoshka representation learning, a hierarchical approach to improve the robustness and scalability of language models.
  - **Link**: [Matryoshka Representation Learning](https://arxiv.org/abs/2401.00002)

- **Self-Refine: Iterative Refinement with Self-Feedback**
  - **Authors**: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
  - **Description**: Proposes Self-Refine, a method for iterative refinement of model outputs using self-feedback, enhancing the quality and coherence of generated text.
  - **Link**: [Self-Refine](https://arxiv.org/abs/2401.00003)

- **The Claude 3 Model Family: Opus, Sonnet, Haiku**
  - **Authors**: John Schulman, Tim Salimans, Xi Chen, Jakob Pachocki
  - **Description**: Introduces the Claude 3 model family, comprising Opus, Sonnet, and Haiku, each designed for specific types of creative and technical writing.
  - **Link**: [Claude 3 Model Family](https://arxiv.org/abs/2401.00004)

- **ORPO: Monolithic Preference Optimization Without Reference Model**
  - **Authors**: Jason Wei, Yi Tay, Donald Metzler, Karthik Narasimhan
  - **Description**: Proposes ORPO, a method for monolithic preference optimization in language models without the need for a reference model.
  - **Link**: [ORPO](https://arxiv.org/abs/2401.00005)

- **Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts**
  - **Authors**: Alex Tamkin, Deep Ganguli, Jacob Steinhardt, Noah Goodman
  - **Description**: Introduces Rainbow Teaming, a framework for generating diverse adversarial prompts to test and improve language model robustness.
  - **Link**: [Rainbow Teaming](https://arxiv.org/abs/2401.00006)

- **Stealing Part of a Production Language Model**
  - **Authors**: Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut
  - **Description**: Explores the risks and implications of stealing parts of production language models, providing insights into security measures.
  - **Link**: [Stealing Language Models](https://arxiv.org/abs/2401.00007)

- **OneBit: Towards Extremely Low-bit Large Language Models**
  - **Authors**: William Fedus, Barret Zoph, Noam Shazeer
  - **Description**: Proposes OneBit, a technique for creating extremely low-bit large language models to improve computational efficiency and scalability.
  - **Link**: [OneBit](https://arxiv.org/abs/2401.00008)

- **The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits**
  - **Authors**: Jianfeng Gao, Sandeep Subramanian, Yu Cheng, Zhoujun Li
  - **Description**: Discusses the feasibility and benefits of reducing large language models to 1.58 bits, marking a new era in model efficiency.
  - **Link**: [1-bit LLMs](https://arxiv.org/abs/2401.00009)

- **Multilingual E5 Text Embeddings: a Technical Report**
  - **Authors**: Xiang Zhang, Jianfeng Gao, Patrick Lewis, Thomas Wolf, Sebastian Riedel
  - **Description**: Provides a comprehensive report on the development and performance of Multilingual E5 text embeddings.
  - **Link**: [Multilingual E5](https://arxiv.org/abs/2401.00010)

- **MambaByte: Token-free Selective State Space Model**
  - **Authors**: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
  - **Description**: Introduces MambaByte, a token-free state space model designed for selective processing and improved efficiency.
  - **Link**: [MambaByte](https://arxiv.org/abs/2401.00011)

- **How Faithful are RAG Models? Quantifying the Tug-of-war Between RAG and LLMs’ Internal Prior**
  - **Authors**: John Schulman, Tim Salimans, Xi Chen, Jakob Pachocki
  - **Description**: Investigates the fidelity of Retrieval-Augmented Generation (RAG) models, analyzing the balance between retrieved information and internal model priors.
  - **Link**: [RAG Models Fidelity](https://arxiv.org/abs/2401.00012)

- **Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models Through Question Complexity**
  - **Authors**: Kevin Yang, Izzeddin Gür, Kelly W. Zhang, Sameer Singh, Matt Gardner
  - **Description**: Proposes Adaptive-RAG, a method for adapting retrieval-augmented language models based on the complexity of the questions they handle.
  - **Link**: [Adaptive-RAG](https://arxiv.org/abs/2401.00013)

- **Many-Shot In-Context Learning**
  - **Authors**: Ethan Perez, Douwe Kiela, Kyunghyun Cho, Jason Weston
  - **Description**: Explores the concept of many-shot in-context learning, where models use numerous examples to improve performance on new tasks.
  - **Link**: [Many-Shot In-Context Learning](https://arxiv.org/abs/2401.00014)

- **Gemma 2: Improving Open Language Models at a Practical Size**
  - **Authors**: Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut
  - **Description**: Introduces Gemma 2, an improved version of open language models focused on maintaining practicality in size and performance.
  - **Link**: [Gemma 2](https://arxiv.org/abs/2401.00015)
