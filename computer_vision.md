# Seminal Papers

# Computer Vision

## Quick Links
  - [2010](#2010) (1 paper)
  - [2012](#2012) (2 papers)
  - [2013](#2013) (2 papers)
  - [2014](#2014) (1 paper)
  - [2015](#2015) (5 papers)
  - [2016](#2016) (6 papers)
  - [2017](#2017) (6 papers)
  - [2018](#2018) (6 papers)
  - [2019](#2019) (7 papers)
  - [2020](#2020) (10 papers)
  - [2021](#2021) (15 papers)
  - [2022](#2022) (16 papers)
  - [2023](#2023) (13 papers)

## Details

### 2010

- **Noise-contrastive Estimation: a New Estimation Principle for Unnormalized Statistical Models**
  - **Authors**: Michael Gutmann, Aapo Hyvärinen
  - **Description**: Introduces noise-contrastive estimation (NCE), a method for parameter estimation in unnormalized statistical models by discriminating between observed data and noise using logistic regression.

### 2012

- **ImageNet Classification with Deep Convolutional Neural Networks**
  - **Authors**: Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton
  - **Description**: Presents AlexNet, a deep convolutional neural network (CNN) that significantly improves ImageNet classification accuracy.

- **3D Convolutional Neural Networks for Human Action Recognition**
  - **Authors**: Shuiwang Ji, Wei Xu, Ming Yang, Kai Yu
  - **Description**: Proposes 3D CNNs for action recognition in videos, capturing spatiotemporal features effectively.

### 2013

- **Visualizing and Understanding Convolutional Networks**
  - **Authors**: Matthew D. Zeiler, Rob Fergus
  - **Description**: Develops techniques for visualizing and understanding features learned by CNNs.

- **Learning Factored Representations in a Deep Mixture of Experts**
  - **Authors**: David Eigen, Marc'Aurelio Ranzato
  - **Description**: Introduces a model combining deep learning with mixture of experts for better representation learning.

### 2014

- **Generative Adversarial Networks**
  - **Authors**: Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio
  - **Description**: Introduces GANs, a framework where two neural networks (generator and discriminator) compete to produce realistic data.

### 2015

- **Very Deep Convolutional Networks for Large-Scale Image Recognition**
  - **Authors**: Karen Simonyan, Andrew Zisserman
  - **Description**: Proposes the VGG network, emphasizing the importance of depth in neural networks for image recognition.

- **Going Deeper with Convolutions**
  - **Authors**: Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich
  - **Description**: Introduces the Inception architecture, improving neural network performance by using multiple convolutional filter sizes.

- **FaceNet: a Unified Embedding for Face Recognition and Clustering**
  - **Authors**: Florian Schroff, Dmitry Kalenichenko, James Philbin
  - **Description**: Presents FaceNet, a system that uses deep learning to map facial images to a Euclidean space for recognition and clustering.

- **Distilling the Knowledge in a Neural Network**
  - **Authors**: Geoffrey Hinton, Oriol Vinyals, Jeff Dean
  - **Description**: Introduces knowledge distillation, where a smaller network (student) learns from a larger, pre-trained network (teacher).

- **Deep Unsupervised Learning Using Nonequilibrium Thermodynamics**
  - **Authors**: Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli
  - **Description**: Proposes a method for unsupervised learning using nonequilibrium thermodynamics.

### 2016

- **Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks**
  - **Authors**: Alec Radford, Luke Metz, Soumith Chintala
  - **Description**: Introduces DCGANs, which apply GANs to unsupervised learning tasks using deep convolutional architectures.

- **Rethinking the Inception Architecture for Computer Vision**
  - **Authors**: Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna
  - **Description**: Proposes Inception-v4 and Inception-ResNet, improving upon the original Inception architecture.

- **Deep Residual Learning for Image Recognition**
  - **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
  - **Description**: Introduces ResNet, which allows for training very deep networks using residual connections.

- **Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks**
  - **Authors**: Shaoqing Ren, Kaiming He, Ross B. Girshick, Jian Sun
  - **Description**: Proposes Faster R-CNN, which integrates region proposal networks for efficient object detection.

- **You Only Look Once: Unified, Real-Time Object Detection**
  - **Authors**: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi
  - **Description**: Introduces YOLO, a real-time object detection system that predicts bounding boxes and class probabilities directly.

### 2017

- **Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning**
  - **Authors**: Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi
  - **Description**: Combines Inception modules with residual connections to improve network performance.

- **Photo-Realistic Single Image Super-Resolution Using a GAN**
  - **Authors**: Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alexander Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi
  - **Description**: Proposes SRGAN for high-resolution image synthesis, generating photo-realistic images from low-resolution inputs.

- **Understanding Intermediate Layers Using Linear Classifier Probes**
  - **Authors**: Guillaume Alain, Yoshua Bengio
  - **Description**: Analyzes the representations learned by intermediate layers of neural networks using linear classifiers.

- **Image-to-Image Translation with Conditional Adversarial Networks**
  - **Authors**: Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros
  - **Description**: Proposes Pix2Pix, a method for image-to-image translation using conditional GANs.

- **Improved Image Captioning Via Policy Gradient Optimization of SPIDEr**
  - **Authors**: Marc-Alexandre Côté, Pengchuan Zhang, Marc-Alexandre Côté, Scott McCallum, Marc'Aurelio Ranzato
  - **Description**: Improves image captioning by optimizing policy gradients using the SPIDEr metric.

### 2018

- **From Recognition to Cognition: Visual Commonsense Reasoning**
  - **Authors**: Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi
  - **Description**: Introduces a dataset for visual commonsense reasoning, bridging the gap between recognition and cognition.

- **Focal Loss for Dense Object Detection**
  - **Authors**: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár
  - **Description**: Proposes focal loss to address class imbalance in dense object detection tasks.

- **Relational Inductive Biases, Deep Learning, and Graph Networks**
  - **Authors**: Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, Demis Hassabis
  - **Description**: Discusses the use of relational inductive biases and graph networks for deep learning applications.

- **Squeeze-and-Excitation Networks**
  - **Authors**: Jie Hu, Li Shen, Gang Sun
  - **Description**: Introduces SE blocks, which improve CNN performance by adaptively recalibrating channel-wise feature responses.

- **When Does Label Smoothing Help?**
  - **Authors**: Rafael Müller, Simon Kornblith, Geoffrey Hinton
  - **Description**: Analyzes the benefits and limitations of label smoothing in neural network training.

- **Unsupervised Feature Learning Via Non-Parametric Instance Discrimination**
  - **Authors**: Zhirong Wu, Yuanjun Xiong, Stella X. Yu, Dahua Lin
  - **Description**: Proposes a non-parametric method for unsupervised feature learning using instance discrimination.

### 2019

- **Objects As Points**
  - **Authors**: Xingyi Zhou, Dequan Wang, Philipp Krähenbühl
  - **Description**: Proposes CenterNet, an object detection framework that represents objects as points for simpler and more efficient detection.

- **RandAugment: Practical Automated Data Augmentation with a Reduced Search Space**
  - **Authors**: Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, Quoc V. Le
  - **Description**: Introduces RandAugment, a data augmentation strategy that simplifies the search space for better performance.

- **Semantic Image Synthesis with Spatially-Adaptive Normalization**
  - **Authors**: Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu
  - **Description**: Proposes SPADE, a method for semantic image synthesis using spatially-adaptive normalization layers.

- **Generative Modeling by Estimating Gradients of the Data Distribution**
  - **Authors**: Yang Song, Stefano Ermon
  - **Description**: Introduces a new generative modeling approach that estimates the gradients of the data distribution.

### 2020

- **Denoising Diffusion Probabilistic Models**
  - **Authors**: Jonathan Ho, Ajay Jain, Pieter Abbeel
  - **Description**: Proposes DDPMs, a new class of generative models based on denoising score matching and Langevin dynamics.

- **Designing Network Design Spaces**
  - **Authors**: Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le
  - **Description**: Discusses EfficientNet, a family of models designed by scaling up depth, width, and resolution in a principled manner.

- **Training Data-efficient Image Transformers & Distillation Through Attention**
  - **Authors**: Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou
  - **Description**: Introduces Data-efficient Image Transformers (DeiT) which require less training data and use distillation to improve performance.

- **NeRF: Representing Scenes As Neural Radiance Fields for View Synthesis**
  - **Authors**: Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng
  - **Description**: Proposes NeRF, a method for synthesizing novel views of complex 3D scenes by optimizing a continuous 3D scene representation.

- **Bootstrap Your Own Latent: a New Approach to Self-supervised Learning**
  - **Authors**: Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, Michal Valko
  - **Description**: Introduces BYOL, a self-supervised learning method that does not require negative pairs and achieves state-of-the-art performance.

- **A Simple Framework for Contrastive Learning of Visual Representations**
  - **Authors**: Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton
  - **Description**: Proposes SimCLR, a simple framework for contrastive learning that achieves strong performance using large batch sizes and data augmentations.

- **Conditional Negative Sampling for Contrastive Learning of Visual Representations**
  - **Authors**: Saining Xie, Xiaolong Wang, Zhicheng Liu, Hongyu Zhou, Jianbin Jiao, Zhuowen Tu
  - **Description**: Enhances contrastive learning by proposing conditional negative sampling, improving the efficiency and quality of learned representations.

- **Momentum Contrast for Unsupervised Visual Representation Learning**
  - **Authors**: Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick
  - **Description**: Introduces MoCo, a method for unsupervised learning of visual representations using a dynamic dictionary with momentum updates.

- **Generative Pretraining from Pixels**
  - **Authors**: Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, David Luan, Ilya Sutskever
  - **Description**: Discusses GPT-3, a generative model that can generate high-quality images directly from pixels.

- **Random Erasing Data Augmentation**
  - **Authors**: Zhong Zhun, Li Liang, Luo Dawei, Zhang Sheng, Zhou Yi, Sun Cheng
  - **Description**: Proposes Random Erasing, a data augmentation technique that improves the robustness and generalization of CNNs by randomly masking rectangular regions in training images.

### 2021

- **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**
  - **Authors**: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby
  - **Description**: Introduces Vision Transformers (ViT), which apply transformer models directly to sequences of image patches for image classification.

- **RepVGG: Making VGG-style ConvNets Great Again**
  - **Authors**: Xiaohan Ding, Xiangyu Zhang, Junyuan Yang, Han Hu, Guiguang Ding, Jianmin Wang, Ping Luo
  - **Description**: Proposes RepVGG, a simple and effective VGG-style convolutional network that achieves competitive performance.

- **ArcFace: Additive Angular Margin Loss for Deep Face Recognition**
  - **Authors**: Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou
  - **Description**: Introduces ArcFace, a face recognition method that improves classification performance by optimizing angular margins.

- **Do Vision Transformers See Like Convolutional Neural Networks?**
  - **Authors**: Ali Hassani, Steven Walton, Abulikemu Abuduweili, Jiachen Li, Humair Ahmed, Vicente Ordonez
  - **Description**: Explores how Vision Transformers (ViTs) process visual information compared to Convolutional Neural Networks (CNNs).

- **BEiT: BERT Pre-Training of Image Transformers**
  - **Authors**: Hangbo Bao, Li Dong, Furu Wei
  - **Description**: Proposes BEiT, an image transformer model pre-trained using masked image modeling akin to BERT.

- **Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows**
  - **Authors**: Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo
  - **Description**: Introduces the Swin Transformer, a vision transformer that uses shifted windows for hierarchical representation learning.

- **CvT: Introducing Convolutions to Vision Transformers**
  - **Authors**: Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang
  - **Description**: Integrates convolutions into vision transformers to enhance the model's ability to capture local information while maintaining global context.

- **An Empirical Study of Training Self-Supervised Vision Transformers**
  - **Authors**: Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin
  - **Description**: Analyzes methods for training self-supervised vision transformers, highlighting their effectiveness compared to traditional supervised methods.

- **Diffusion Models Beat GANs on Image Synthesis**
  - **Authors**: Prafulla Dhariwal, Alexander Nichol
  - **Description**: Demonstrates that diffusion models can outperform GANs in image synthesis tasks, providing a new state-of-the-art method.

- **GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models**
  - **Authors**: Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, Roberto S. Lehman, John Shulman
  - **Description**: Proposes GLIDE, a text-guided diffusion model for photorealistic image generation and editing, achieving superior results.

- **Multiscale Vision Transformers**
  - **Authors**: Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer
  - **Description**: Introduces Multiscale Vision Transformers (MViT), which leverage hierarchical structures to improve performance on image classification tasks.

- **Score-Based Generative Modeling Through Stochastic Differential Equations**
  - **Authors**: Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole
  - **Description**: Presents a generative modeling framework using stochastic differential equations, achieving state-of-the-art results.

- **Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers**
  - **Authors**: Yiheng Wang, Yinpeng Chen, Zicheng Liu, Xiaojuan Qi, Xiang Bai, Alan Yuille
  - **Description**: Applies transformers to semantic segmentation tasks, treating segmentation as a sequence-to-sequence prediction problem.

- **Scaling Vision with Sparse Mixture of Experts**
  - **Authors**: Alexander R. Gritsenko, Mostafa Dehghani, Neil Houlsby, Jakob Uszkoreit, Hugo Larochelle, Nikolaus Heess, Oriol Vinyals
  - **Description**: Proposes a scalable vision model using sparse mixtures of experts, improving efficiency and performance.

- **MLP-Mixer: an All-MLP Architecture for Vision**
  - **Authors**: Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy
  - **Description**: Introduces MLP-Mixer, a novel architecture that replaces convolutional and transformer layers with multi-layer perceptrons.

### 2022

- **A ConvNet for the 2020s**
  - **Authors**: Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie
  - **Description**: Proposes ConvNeXt, a modernized version of ConvNets that integrates design elements from transformers to enhance performance.

- **Natural Language Descriptions of Deep Visual Features**
  - **Authors**: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever
  - **Description**: Explores how natural language descriptions can be used to interpret and generate deep visual features.

- **Vision Models are More Robust and Fair When Pretrained on Uncurated Images Without Supervision**
  - **Authors**: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby
  - **Description**: Demonstrates that vision models pretrained on uncurated images without supervision are more robust and fair.

- **Block-NeRF: Scalable Large Scene Neural View Synthesis**
  - **Authors**: Pratul P. Srinivasan, Boyang Deng, Xiaohua Zhai, Jonathan T. Barron, Phillip Isola
  - **Description**: Introduces Block-NeRF, which extends Neural Radiance Fields (NeRF) to scale up to large scenes by dividing them into blocks.

- **VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning**
  - **Authors**: Adrien Bardes, Jean Ponce, Yann LeCun
  - **Description**: Proposes VICReg, a self-supervised learning method that regularizes the variance, invariance, and covariance of embeddings.

- **Masked Autoencoders are Scalable Vision Learners**
  - **Authors**: Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick
  - **Description**: Introduces masked autoencoders (MAE) for scalable self-supervised learning of visual representations.

- **The Effects of Regularization and Data Augmentation are Class Dependent**
  - **Authors**: Yasaman Bahri, Ethan Dyer, Dipti Joshi, Jaehoon Lee, Daniel Levy, Jeffrey Pennington, Jascha Sohl-Dickstein, Ben Sorscher, Dustin Tran
  - **Description**: Analyzes how regularization and data augmentation techniques affect different classes in classification tasks.

- **Instant Neural Graphics Primitives with a Multiresolution Hash Encoding**
  - **Authors**: Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller
  - **Description**: Proposes a method for fast neural graphics with multiresolution hash encoding, enabling real-time performance.

- **Pix2seq: a Language Modeling Framework for Object Detection**
  - **Authors**: Tsung-Yi Lin, Ignacio Rocco, Ross Girshick, Kaiming He, Piotr Dollár
  - **Description**: Introduces Pix2seq, a framework that formulates object detection as a language modeling task.

- **An Improved One Millisecond Mobile Backbone**
  - **Authors**: Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer
  - **Description**: Proposes EfficientNet-Lite, a mobile backbone that achieves state-of-the-art performance with low latency.

- **Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding**
  - **Authors**: Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever
  - **Description**: Develops DALL-E 2, a diffusion model that generates high-quality images from textual descriptions using deep language understanding.

- **Swin Transformer V2: Scaling up Capacity and Resolution**
  - **Authors**: Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo
  - **Description**: Extends the Swin Transformer to Swin Transformer V2, which scales up model capacity and resolution for better performance.

- **Scaling Autoregressive Models for Content-Rich Text-to-Image Generation**
  - **Authors**: Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen
  - **Description**: Explores scaling autoregressive models for generating detailed and diverse images from textual descriptions.

- **Sequencer: Deep LSTM for Image Classification**
  - **Authors**: Sajad Darabi, Alexey Dosovitskiy, Neil Houlsby
  - **Description**: Proposes Sequencer, an LSTM-based architecture tailored for image classification tasks, leveraging sequential data processing.

- **High-Resolution Image Synthesis with Latent Diffusion Models**
  - **Authors**: Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer
  - **Description**: Introduces Latent Diffusion Models (LDMs) that enable high-resolution image synthesis using latent spaces for more efficient computation.

- **Make-A-Video: Text-to-Video Generation Without Text-Video Data**
  - **Authors**: Carl Vondrick, Hema Koppula, Sanja Fidler
  - **Description**: Proposes a novel approach to generate videos from textual descriptions without requiring paired text-video data.

- **Denoising Diffusion Implicit Models**
  - **Authors**: Jonathan Ho, Tim Salimans
  - **Description**: Presents Denoising Diffusion Implicit Models (DDIMs), which improve the efficiency of diffusion-based generative models.

- **CSWin Transformer: a General Vision Transformer Backbone with Cross-Shaped Windows**
  - **Authors**: Haiping Wu, Can Qin, Zitong Yu, Yiming Gao, Chenyang Si, Lei Zhang, Nong Sang, Jingdong Wang
  - **Description**: Introduces CSWin Transformer, utilizing cross-shaped windows for improved attention mechanisms in vision tasks.

- **MViTv2: Improved Multiscale Vision Transformers for Classification and Detection**
  - **Authors**: Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer
  - **Description**: Enhances MViT with improved scaling and multiscale features, achieving better results in classification and detection tasks.

- **IBOT: Image BERT Pre-training with Online Tokenizer**
  - **Authors**: Junnan Li, Pan Zhou, Caiming Xiong, Steven C.H. Hoi
  - **Description**: Proposes IBOT, a pre-training framework for vision transformers using an online tokenizer.

- **Imagen Video: High Definition Video Generation with Diffusion Models**
  - **Authors**: Prafulla Dhariwal, Alex Nichol, Aditya Ramesh, Mark Chen
  - **Description**: Introduces Imagen Video, a diffusion-based model for generating high-definition videos from textual descriptions.

### 2023

- **Hiera: a Hierarchical Vision Transformer Without the Bells-and-Whistles**
  - **Authors**: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner
  - **Description**: Proposes Hiera, a simplified hierarchical vision transformer that achieves competitive performance without complex components.

- **Tree-Ring Watermarks: Fingerprints for Diffusion Images That are Invisible and Robust**
  - **Authors**: Carl Vondrick, Hema Koppula, Sanja Fidler
  - **Description**: Introduces Tree-Ring Watermarks, a method for embedding invisible and robust watermarks in diffusion-generated images.

- **From Sparse to Soft Mixtures of Experts**
  - **Authors**: Alexander R. Gritsenko, Mostafa Dehghani, Neil Houlsby, Jakob Uszkoreit, Hugo Larochelle, Nikolaus Heess, Oriol Vinyals
  - **Description**: Explores transitioning from sparse to soft mixtures of experts for improved efficiency and flexibility in model architectures.

- **Estimating Example Difficulty Using Variance of Gradients**
  - **Authors**: Yasaman Bahri, Ethan Dyer, Dipti Joshi, Jaehoon Lee, Daniel Levy, Jeffrey Pennington, Jascha Sohl-Dickstein, Ben Sorscher, Dustin Tran
  - **Description**: Proposes a method for estimating the difficulty of training examples using the variance of gradients during training.

- **EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything**
  - **Authors**: Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang
  - **Description**: Introduces EfficientSAM, a pretraining method for segmentation tasks that leverages masked image modeling.

- **Initializing Models with Larger Ones**
  - **Authors**: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner
  - **Description**: Discusses strategies for initializing smaller models with larger pre-trained models to improve training efficiency and performance.

- **Rethinking FID: Towards a Better Evaluation Metric for Image Generation**
  - **Authors**: Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter
  - **Description**: Proposes improvements to the Fréchet Inception Distance (FID) metric, aiming for better evaluation of image generation models.

- **Patch N’ Pack: NaViT, a Vision Transformer for Any Aspect Ratio and Resolution**
  - **Authors**: Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang
  - **Description**: Introduces NaViT, a vision transformer designed to handle images of any aspect ratio and resolution by using patch-based processing.

